{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 143.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 4)\n",
      "(100, 17)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "80/80 [==============================] - 2s - loss: 2.7319 - acc: 0.5169 - val_loss: 1.9715 - val_acc: 0.7794\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 0s - loss: 2.7753 - acc: 0.7213 - val_loss: 0.9994 - val_acc: 0.8912\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 0s - loss: 1.8996 - acc: 0.8044 - val_loss: 1.0092 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 0s - loss: 1.4499 - acc: 0.8463 - val_loss: 1.0416 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 0s - loss: 1.3659 - acc: 0.8419 - val_loss: 0.8557 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 1s - loss: 1.1565 - acc: 0.8382 - val_loss: 0.6083 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 0s - loss: 0.8466 - acc: 0.8507 - val_loss: 0.3654 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "80/80 [==============================] - 0s - loss: 0.5036 - acc: 0.8500 - val_loss: 0.3613 - val_acc: 0.8500\n",
      "Epoch 9/20\n",
      "80/80 [==============================] - 0s - loss: 0.4492 - acc: 0.7875 - val_loss: 0.4244 - val_acc: 0.8412\n",
      "Epoch 10/20\n",
      "80/80 [==============================] - 0s - loss: 0.4737 - acc: 0.7816 - val_loss: 0.4003 - val_acc: 0.8794\n",
      "Epoch 11/20\n",
      "80/80 [==============================] - 0s - loss: 0.4394 - acc: 0.8184 - val_loss: 0.3267 - val_acc: 0.8941\n",
      "Epoch 12/20\n",
      "80/80 [==============================] - 0s - loss: 0.3766 - acc: 0.8456 - val_loss: 0.2859 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "80/80 [==============================] - 0s - loss: 0.3603 - acc: 0.8654 - val_loss: 0.3036 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "80/80 [==============================] - 1s - loss: 0.4097 - acc: 0.8544 - val_loss: 0.2997 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "80/80 [==============================] - 1s - loss: 0.3562 - acc: 0.8743 - val_loss: 0.2848 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "80/80 [==============================] - 1s - loss: 0.3435 - acc: 0.8662 - val_loss: 0.2824 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "80/80 [==============================] - 0s - loss: 0.3390 - acc: 0.8676 - val_loss: 0.2947 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "80/80 [==============================] - 0s - loss: 0.3308 - acc: 0.8699 - val_loss: 0.3098 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "80/80 [==============================] - 1s - loss: 0.3363 - acc: 0.8662 - val_loss: 0.3069 - val_acc: 0.8912\n",
      "Epoch 20/20\n",
      "80/80 [==============================] - 0s - loss: 0.3271 - acc: 0.8684 - val_loss: 0.2925 - val_acc: 0.8971\n",
      "[[1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]]\n",
      "[[ 0.37351143  0.06131607  0.11413418  0.09341542  0.15222634  0.11224154\n",
      "   0.15040459  0.43239471  0.66577184  0.19555785  0.07619081  0.87251383\n",
      "   0.24786708  0.17324704  0.08684991  0.45172113  0.30152273]\n",
      " [ 0.33169878  0.01226478  0.02793479  0.02238915  0.05861122  0.11684714\n",
      "   0.1024699   0.29021788  0.83094627  0.0316355   0.02171663  0.9796865\n",
      "   0.15946734  0.1163777   0.02674337  0.15898538  0.14668812]\n",
      " [ 0.36086255  0.01943009  0.04527901  0.03806002  0.08432093  0.11747532\n",
      "   0.1189031   0.32582861  0.73767406  0.0611987   0.03317066  0.95856988\n",
      "   0.199589    0.14075877  0.042739    0.2382066   0.18091485]\n",
      " [ 0.3580609   0.01842218  0.03926223  0.03156501  0.08103168  0.12207934\n",
      "   0.11014907  0.32128587  0.73426193  0.05464391  0.02833738  0.96640122\n",
      "   0.18991756  0.13792792  0.04714428  0.1934192   0.17032491]\n",
      " [ 0.40171498  0.03840376  0.0897387   0.06179664  0.12864149  0.11750657\n",
      "   0.1273817   0.4133141   0.68703759  0.10496465  0.05181722  0.92158353\n",
      "   0.21912214  0.19367594  0.07527237  0.28199321  0.22678366]\n",
      " [ 0.39948386  0.0638146   0.11671968  0.10616443  0.16590394  0.10336948\n",
      "   0.14737017  0.44420838  0.64809728  0.16263901  0.07746833  0.86491054\n",
      "   0.22774388  0.18021062  0.07839473  0.43804362  0.28551275]\n",
      " [ 0.35643482  0.0163916   0.0325775   0.02593024  0.05915158  0.13484937\n",
      "   0.12776513  0.28058797  0.84859717  0.02996395  0.0266034   0.97731656\n",
      "   0.17075619  0.1261224   0.02909946  0.15921183  0.14602028]\n",
      " [ 0.27652088  0.01271348  0.03238917  0.05748207  0.06448046  0.02199037\n",
      "   0.0750833   0.33330348  0.83034813  0.03454524  0.00971623  0.96980035\n",
      "   0.06600656  0.09036337  0.01177174  0.40949389  0.15300436]\n",
      " [ 0.29032156  0.00592337  0.0182012   0.01161821  0.03583545  0.11527678\n",
      "   0.08858896  0.33187979  0.93722826  0.00946974  0.01472439  0.99456936\n",
      "   0.11726853  0.08524744  0.00868869  0.09971352  0.10294282]\n",
      " [ 0.36966166  0.020422    0.0461834   0.03546376  0.08586575  0.14133812\n",
      "   0.12402816  0.32114267  0.78380024  0.04740594  0.03440908  0.96964139\n",
      "   0.21984075  0.14946643  0.04222244  0.19284956  0.17628537]\n",
      " [ 0.36510459  0.02434446  0.04516554  0.03876546  0.09797802  0.1095259\n",
      "   0.11872645  0.3526921   0.6977222   0.07223883  0.03386863  0.95090163\n",
      "   0.16586415  0.13755906  0.05190013  0.2914325   0.18537582]\n",
      " [ 0.30261907  0.00736045  0.01708767  0.01175146  0.02837631  0.10644135\n",
      "   0.09298413  0.26479459  0.91341013  0.01408565  0.01241692  0.99091786\n",
      "   0.10077394  0.07954523  0.01337763  0.11133911  0.10539696]\n",
      " [ 0.38949928  0.0349441   0.07772712  0.06496403  0.09828158  0.17233008\n",
      "   0.13395797  0.37657589  0.76853132  0.06862734  0.05233536  0.95947021\n",
      "   0.23860486  0.18418865  0.05814058  0.19943412  0.21224047]\n",
      " [ 0.37123805  0.02160707  0.04484334  0.03297724  0.07001352  0.1556845\n",
      "   0.13492891  0.31592116  0.84062225  0.03646399  0.03254747  0.97403085\n",
      "   0.19289674  0.14318822  0.03640005  0.1540307   0.15546212]\n",
      " [ 0.29389539  0.01384235  0.03859923  0.02963523  0.08800785  0.15704972\n",
      "   0.09734849  0.37921605  0.78706414  0.0445402   0.0322201   0.97586006\n",
      "   0.20864548  0.16488342  0.04469584  0.18320499  0.17604584]\n",
      " [ 0.20590548  0.0060883   0.01758464  0.01417007  0.02496242  0.13255861\n",
      "   0.08506672  0.38934129  0.90809327  0.02196271  0.01460923  0.99172473\n",
      "   0.11151142  0.0971619   0.01413025  0.09497226  0.08896599]\n",
      " [ 0.3272638   0.00918391  0.02318498  0.0173109   0.04614511  0.11278258\n",
      "   0.0852172   0.28197297  0.81169808  0.02645532  0.01699478  0.98581886\n",
      "   0.16640365  0.10380345  0.02381426  0.12474839  0.12615776]\n",
      " [ 0.40676275  0.113337    0.15839967  0.14287291  0.18607883  0.10626952\n",
      "   0.21077864  0.44112098  0.68366539  0.23230533  0.12001007  0.78026456\n",
      "   0.26832917  0.23280478  0.09618238  0.55273443  0.36296743]\n",
      " [ 0.30534941  0.00741471  0.01838177  0.01295734  0.03509577  0.1021077\n",
      "   0.09259106  0.26046249  0.92254025  0.01074304  0.01355366  0.99198711\n",
      "   0.12930138  0.08325301  0.01089843  0.0956525   0.0973332 ]\n",
      " [ 0.35352772  0.01381051  0.0433975   0.0328537   0.06319367  0.10998641\n",
      "   0.09715359  0.33317637  0.86405253  0.03098086  0.02582817  0.9775213\n",
      "   0.16248967  0.13645099  0.02215683  0.16774052  0.17165248]]\n",
      "0.680766177463\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('../input/amazon/train.csv')\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = tiff.imread('../input/amazon/train-tif/{}.tif'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train.append(cv2.resize(img, (32, 32)))\n",
    "    y_train.append(targets)\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float16) / 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# split = 35000\n",
    "split = 80\n",
    "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32, 32, 4)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          callbacks=[ModelCheckpoint('keras-simple.model', monitor='val_loss', verbose=0, mode='auto', period=1)],\n",
    "          validation_data=(x_valid, y_valid))\n",
    "          \n",
    "p_valid = model.predict(x_valid, batch_size=128)\n",
    "print(y_valid)\n",
    "print(p_valid)\n",
    "                \n",
    "from sklearn.metrics import fbeta_score\n",
    "def f2_score(y_true, y_pred):\n",
    "    # fbeta_score throws a confusing error if inputs are not numpy arrays\n",
    "    y_true, y_pred, = np.array(y_true), np.array(y_pred)\n",
    "    # We need to use average='samples' here, any other average method will generate bogus results\n",
    "    return fbeta_score(y_true, y_pred, beta=2, average='samples')\n",
    "                     \n",
    "print(f2_score(y_valid, np.array(p_valid) > 0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
