{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/amazon/'\n",
    "TRAIN_TIF_DIR = DATA_DIR + 'train-tif/'\n",
    "TRAIN_CSV = DATA_DIR + 'train.csv'\n",
    "TEST_TIF_DIR = DATA_DIR + 'test-tif/'\n",
    "\n",
    "IMG_SIZE = 227\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'amazon-{}-{}.model'.format(LR, 'alexnet')\n",
    "\n",
    "CLOUD_COVER_LABELS = [\n",
    "    'clear', \n",
    "    'cloudy', \n",
    "    'haze', \n",
    "    'partly_cloudy']\n",
    "\n",
    "# read our data and take a look at what we are dealing with\n",
    "train_csv = pd.read_csv(TRAIN_CSV)\n",
    "train_csv.head()\n",
    "\n",
    "tags = pd.DataFrame()\n",
    "\n",
    "for label in CLOUD_COVER_LABELS:\n",
    "    tags[label] = train_csv.tags.apply(lambda x: np.where(label in x, 1, 0))\n",
    "    \n",
    "train_csv = pd.concat([train_csv, tags], axis=1)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "train = pd.concat([train_csv[train_csv.clear == 1].sample(n=7251),\n",
    "    train_csv[train_csv.cloudy == 1].sample(n=7251),\n",
    "    train_csv[train_csv.haze == 1],\n",
    "    train_csv[train_csv.partly_cloudy == 1].sample(n=7251)], axis=0, ignore_index=True)\n",
    "\n",
    "del train_csv\n",
    "del tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    # fbeta_score throws a confusing error if inputs are not numpy arrays\n",
    "    y_true, y_pred, = np.array(y_true), np.array(y_pred)\n",
    "    # We need to use average='samples' here, any other average method will generate bogus results\n",
    "    return fbeta_score(y_true, y_pred, beta=2, average='samples')\n",
    "\n",
    "# convert cloud cover labels to array [clear, cloudy, haze, partly_cloudy]\n",
    "def get_cloud_cover_labels(row):\n",
    "    labels = np.array([row.clear, row.cloudy, row.haze, row.partly_cloudy])\n",
    "    return labels\n",
    "\n",
    "# load image\n",
    "# reduce image from 255,255,4 to 100,100,4\n",
    "# flatten out to 1-D array in order R,G,B,NIR (should we use greyscale instead, ignore NIR?)\n",
    "def load_image(filename):\n",
    "    path = os.path.abspath(os.path.join(TRAIN_TIF_DIR, filename))\n",
    "    if os.path.exists(path):\n",
    "        img = tiff.imread(path)[:,:,:3]\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        return img\n",
    "    # if you reach this line, you didn't find the image you're looking for\n",
    "    print('Load failed: could not find image {}'.format(path))\n",
    "    \n",
    "# create training data from train.csv DataFrame\n",
    "def load_training_data():\n",
    "    train_images = []\n",
    "\n",
    "    for index, row in tqdm(train.iterrows()):\n",
    "        grey_image = load_image(row.image_name + '.tif')\n",
    "        train_images.append([grey_image, \n",
    "                             get_cloud_cover_labels(row),\n",
    "                             row.image_name])\n",
    "\n",
    "    np.save('training_images.npy', train_images)\n",
    "    return train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = load_training_data()\n",
    "\n",
    "train_data = train_images[:-8000]\n",
    "# need a cross validation set\n",
    "cv_data = train_images[-8000:-4000]\n",
    "test_data = train_images[-4000:]\n",
    "\n",
    "X = np.array([i[0] for i in train_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = [i[1] for i in train_data]\n",
    "\n",
    "X_test = np.array([i[0] for i in test_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = [i[1] for i in test_data]\n",
    "\n",
    "X_cv = np.array([i[0] for i in cv_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_cv = [i[1] for i in cv_data]\n",
    "\n",
    "model.fit({'input': X}, {'targets': y}, n_epoch=1000, validation_set=({'input': X_cv}, {'targets': y_cv}), \n",
    "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/output/' + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to measure F2 score instead of accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "score = f2_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
