{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 147.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 32, 32, 4)\n",
      "(20, 4)\n",
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s - loss: 5.5080 - acc: 0.5625 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s - loss: 5.0369 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.7500\n",
      "[[1 0 0 0]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('../../input/amazon/train_v2.csv')\n",
    "\n",
    "AVAILABLE_LABELS = [\n",
    "    'clear', \n",
    "    'cloudy', \n",
    "    'haze', \n",
    "    'partly_cloudy']\n",
    "\n",
    "tags = pd.DataFrame()\n",
    "\n",
    "for label in AVAILABLE_LABELS:\n",
    "    tags[label] = df_train.tags.apply(lambda x: np.where(label in x, 1, 0))\n",
    "    \n",
    "df_train = pd.concat([df_train, tags], axis=1)\n",
    "df_train\n",
    "for f, tags, clear, cloudy, haze, partly_cloudy in tqdm(df_train.values, miniters=1000):\n",
    "    img = tiff.imread('../../input/amazon/train-tif/{}.tif'.format(f))\n",
    "    x_train.append(cv2.resize(img, (32, 32)))\n",
    "    y_train.append([clear, cloudy, haze, partly_cloudy])\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float16) / 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "split = 35000\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32, 32, 4)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          callbacks=[ModelCheckpoint('keras-simple.model', monitor='val_loss', verbose=0, mode='auto', period=1)],\n",
    "          validation_data=(x_valid, y_valid))\n",
    "          \n",
    "p_valid = model.predict(x_valid, batch_size=128)\n",
    "print(y_valid)\n",
    "print(p_valid)\n",
    "                \n",
    "from sklearn.metrics import fbeta_score\n",
    "def f2_score(y_true, y_pred):\n",
    "    # fbeta_score throws a confusing error if inputs are not numpy arrays\n",
    "    y_true, y_pred, = np.array(y_true), np.array(y_pred)\n",
    "    # We need to use average='samples' here, any other average method will generate bogus results\n",
    "    return fbeta_score(y_true, y_pred, beta=2, average='samples')\n",
    "                     \n",
    "print(f2_score(y_valid, p_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # load test data\n",
    "# TEST_DIR = '../input/test-tif-v2/'\n",
    "# test_tif = os.listdir(TEST_DIR)\n",
    "\n",
    "# for file in tqdm(test_tif):\n",
    "#     img = tiff.imread(TEST_DIR + file)\n",
    "#     x_test.append(cv2.resize(img, (32, 32)))\n",
    "\n",
    "# x_test = np.array(x_test, np.float16) / 255.\n",
    "# y_pred = (model.predict(x_test) > 0.2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
